{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c0af06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 22, 1000]             512\n",
      "       BatchNorm2d-2          [-1, 8, 22, 1000]              16\n",
      "            Conv2d-3          [-1, 16, 1, 1000]             352\n",
      "       BatchNorm2d-4          [-1, 16, 1, 1000]              32\n",
      "               ELU-5          [-1, 16, 1, 1000]               0\n",
      "         AvgPool2d-6           [-1, 16, 1, 125]               0\n",
      "           Dropout-7           [-1, 16, 1, 125]               0\n",
      "            Conv2d-8           [-1, 16, 1, 125]           4,096\n",
      "       BatchNorm2d-9           [-1, 16, 1, 125]              32\n",
      "              ELU-10           [-1, 16, 1, 125]               0\n",
      "        AvgPool2d-11            [-1, 16, 1, 15]               0\n",
      "          Dropout-12            [-1, 16, 1, 15]               0\n",
      "        Rearrange-13               [-1, 15, 16]               0\n",
      "PatchEmbeddingCNN-14               [-1, 15, 16]               0\n",
      "          Dropout-15               [-1, 15, 16]               0\n",
      "PositioinalEncoding-16               [-1, 15, 16]               0\n",
      "        LayerNorm-17               [-1, 15, 16]              32\n",
      "            Mamba-18               [-1, 15, 16]               0\n",
      "          Dropout-19               [-1, 15, 16]               0\n",
      "        LayerNorm-20               [-1, 15, 16]              32\n",
      "           Linear-21               [-1, 15, 64]           1,088\n",
      "             GELU-22               [-1, 15, 64]               0\n",
      "          Dropout-23               [-1, 15, 64]               0\n",
      "           Linear-24               [-1, 15, 16]           1,040\n",
      "          Dropout-25               [-1, 15, 16]               0\n",
      "MambaEncoderBlock-26               [-1, 15, 16]               0\n",
      "        LayerNorm-27               [-1, 15, 16]              32\n",
      "            Mamba-28               [-1, 15, 16]               0\n",
      "          Dropout-29               [-1, 15, 16]               0\n",
      "        LayerNorm-30               [-1, 15, 16]              32\n",
      "           Linear-31               [-1, 15, 64]           1,088\n",
      "             GELU-32               [-1, 15, 64]               0\n",
      "          Dropout-33               [-1, 15, 64]               0\n",
      "           Linear-34               [-1, 15, 16]           1,040\n",
      "          Dropout-35               [-1, 15, 16]               0\n",
      "MambaEncoderBlock-36               [-1, 15, 16]               0\n",
      "        LayerNorm-37               [-1, 15, 16]              32\n",
      "            Mamba-38               [-1, 15, 16]               0\n",
      "          Dropout-39               [-1, 15, 16]               0\n",
      "        LayerNorm-40               [-1, 15, 16]              32\n",
      "           Linear-41               [-1, 15, 64]           1,088\n",
      "             GELU-42               [-1, 15, 64]               0\n",
      "          Dropout-43               [-1, 15, 64]               0\n",
      "           Linear-44               [-1, 15, 16]           1,040\n",
      "          Dropout-45               [-1, 15, 16]               0\n",
      "MambaEncoderBlock-46               [-1, 15, 16]               0\n",
      "        LayerNorm-47               [-1, 15, 16]              32\n",
      "            Mamba-48               [-1, 15, 16]               0\n",
      "          Dropout-49               [-1, 15, 16]               0\n",
      "        LayerNorm-50               [-1, 15, 16]              32\n",
      "           Linear-51               [-1, 15, 64]           1,088\n",
      "             GELU-52               [-1, 15, 64]               0\n",
      "          Dropout-53               [-1, 15, 64]               0\n",
      "           Linear-54               [-1, 15, 16]           1,040\n",
      "          Dropout-55               [-1, 15, 16]               0\n",
      "MambaEncoderBlock-56               [-1, 15, 16]               0\n",
      "        LayerNorm-57               [-1, 15, 16]              32\n",
      "            Mamba-58               [-1, 15, 16]               0\n",
      "          Dropout-59               [-1, 15, 16]               0\n",
      "        LayerNorm-60               [-1, 15, 16]              32\n",
      "           Linear-61               [-1, 15, 64]           1,088\n",
      "             GELU-62               [-1, 15, 64]               0\n",
      "          Dropout-63               [-1, 15, 64]               0\n",
      "           Linear-64               [-1, 15, 16]           1,040\n",
      "          Dropout-65               [-1, 15, 16]               0\n",
      "MambaEncoderBlock-66               [-1, 15, 16]               0\n",
      "        LayerNorm-67               [-1, 15, 16]              32\n",
      "            Mamba-68               [-1, 15, 16]               0\n",
      "          Dropout-69               [-1, 15, 16]               0\n",
      "        LayerNorm-70               [-1, 15, 16]              32\n",
      "           Linear-71               [-1, 15, 64]           1,088\n",
      "             GELU-72               [-1, 15, 64]               0\n",
      "          Dropout-73               [-1, 15, 64]               0\n",
      "           Linear-74               [-1, 15, 16]           1,040\n",
      "          Dropout-75               [-1, 15, 16]               0\n",
      "MambaEncoderBlock-76               [-1, 15, 16]               0\n",
      "     MambaEncoder-77               [-1, 15, 16]               0\n",
      "          Flatten-78                  [-1, 240]               0\n",
      "          Dropout-79                  [-1, 240]               0\n",
      "           Linear-80                    [-1, 4]             964\n",
      "================================================================\n",
      "Total params: 19,156\n",
      "Trainable params: 19,156\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.08\n",
      "Forward/backward pass size (MB): 3.35\n",
      "Params size (MB): 0.07\n",
      "Estimated Total Size (MB): 3.51\n",
      "----------------------------------------------------------------\n",
      "Sun Apr 20 11:30:29 2025\n",
      "seed is 2016\n",
      "Subject 1\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "1_0 train_acc: 0.2697 train_loss: 2.484625\tval_acc: 0.279412 val_loss: 1.4982166\n",
      "1_2 train_acc: 0.3146 train_loss: 1.848482\tval_acc: 0.279412 val_loss: 1.4776458\n",
      "1_3 train_acc: 0.3296 train_loss: 1.835673\tval_acc: 0.274510 val_loss: 1.4716111\n",
      "1_4 train_acc: 0.3071 train_loss: 1.747076\tval_acc: 0.333333 val_loss: 1.3550575\n",
      "1_6 train_acc: 0.3184 train_loss: 1.594819\tval_acc: 0.357843 val_loss: 1.2748075\n",
      "1_7 train_acc: 0.3858 train_loss: 1.475011\tval_acc: 0.411765 val_loss: 1.2067074\n",
      "1_8 train_acc: 0.3483 train_loss: 1.470162\tval_acc: 0.509804 val_loss: 1.1338834\n",
      "1_9 train_acc: 0.4195 train_loss: 1.361248\tval_acc: 0.495098 val_loss: 1.0799708\n",
      "1_10 train_acc: 0.4644 train_loss: 1.251505\tval_acc: 0.553922 val_loss: 1.0682262\n",
      "1_11 train_acc: 0.4682 train_loss: 1.219836\tval_acc: 0.651961 val_loss: 0.9823270\n",
      "1_12 train_acc: 0.4831 train_loss: 1.181573\tval_acc: 0.651961 val_loss: 0.9423025\n",
      "1_13 train_acc: 0.4944 train_loss: 1.239961\tval_acc: 0.700980 val_loss: 0.9107549\n",
      "1_14 train_acc: 0.5206 train_loss: 1.117440\tval_acc: 0.720588 val_loss: 0.8742197\n",
      "1_15 train_acc: 0.5281 train_loss: 1.146572\tval_acc: 0.769608 val_loss: 0.7752165\n",
      "1_16 train_acc: 0.5730 train_loss: 1.042582\tval_acc: 0.799020 val_loss: 0.7196237\n",
      "1_17 train_acc: 0.5581 train_loss: 1.051221\tval_acc: 0.799020 val_loss: 0.6709296\n",
      "1_18 train_acc: 0.5805 train_loss: 0.945292\tval_acc: 0.784314 val_loss: 0.6246381\n",
      "1_19 train_acc: 0.6779 train_loss: 0.855778\tval_acc: 0.794118 val_loss: 0.5992952\n",
      "1_20 train_acc: 0.6629 train_loss: 0.766722\tval_acc: 0.784314 val_loss: 0.5581644\n",
      "1_22 train_acc: 0.6816 train_loss: 0.751572\tval_acc: 0.828431 val_loss: 0.4955526\n",
      "1_25 train_acc: 0.6891 train_loss: 0.678800\tval_acc: 0.823529 val_loss: 0.4635725\n",
      "1_28 train_acc: 0.7004 train_loss: 0.722217\tval_acc: 0.843137 val_loss: 0.4244798\n",
      "1_41 train_acc: 0.7453 train_loss: 0.591000\tval_acc: 0.808824 val_loss: 0.4117197\n",
      "1_42 train_acc: 0.7790 train_loss: 0.486052\tval_acc: 0.818627 val_loss: 0.3961755\n",
      "1_49 train_acc: 0.7940 train_loss: 0.505581\tval_acc: 0.862745 val_loss: 0.3581439\n",
      "1_54 train_acc: 0.8015 train_loss: 0.451987\tval_acc: 0.892157 val_loss: 0.2846903\n",
      "1_74 train_acc: 0.8689 train_loss: 0.320274\tval_acc: 0.887255 val_loss: 0.2402360\n",
      "1_81 train_acc: 0.8502 train_loss: 0.347855\tval_acc: 0.911765 val_loss: 0.2395052\n",
      "1_83 train_acc: 0.8689 train_loss: 0.324964\tval_acc: 0.906863 val_loss: 0.2351786\n",
      "1_84 train_acc: 0.8277 train_loss: 0.402832\tval_acc: 0.931373 val_loss: 0.2042831\n",
      "1_90 train_acc: 0.8577 train_loss: 0.384353\tval_acc: 0.926471 val_loss: 0.1869875\n",
      "1_92 train_acc: 0.8352 train_loss: 0.406278\tval_acc: 0.921569 val_loss: 0.1836928\n",
      "1_99 train_acc: 0.8427 train_loss: 0.366650\tval_acc: 0.916667 val_loss: 0.1826371\n",
      "1_103 train_acc: 0.8876 train_loss: 0.285551\tval_acc: 0.941176 val_loss: 0.1781711\n",
      "1_109 train_acc: 0.8801 train_loss: 0.315901\tval_acc: 0.950980 val_loss: 0.1349307\n",
      "1_138 train_acc: 0.8764 train_loss: 0.287973\tval_acc: 0.946078 val_loss: 0.1312911\n",
      "1_145 train_acc: 0.9064 train_loss: 0.243886\tval_acc: 0.946078 val_loss: 0.1304977\n",
      "1_146 train_acc: 0.9064 train_loss: 0.188171\tval_acc: 0.946078 val_loss: 0.1273349\n",
      "1_147 train_acc: 0.8876 train_loss: 0.278352\tval_acc: 0.941176 val_loss: 0.1245263\n",
      "1_148 train_acc: 0.8951 train_loss: 0.233018\tval_acc: 0.955882 val_loss: 0.1090031\n",
      "1_154 train_acc: 0.9064 train_loss: 0.249088\tval_acc: 0.970588 val_loss: 0.0979305\n",
      "1_176 train_acc: 0.9176 train_loss: 0.261162\tval_acc: 0.965686 val_loss: 0.0788224\n",
      "1_205 train_acc: 0.9213 train_loss: 0.223984\tval_acc: 0.975490 val_loss: 0.0727691\n",
      "1_209 train_acc: 0.9213 train_loss: 0.192200\tval_acc: 0.965686 val_loss: 0.0678603\n",
      "1_252 train_acc: 0.9401 train_loss: 0.156975\tval_acc: 0.970588 val_loss: 0.0539796\n",
      "1_254 train_acc: 0.8914 train_loss: 0.337839\tval_acc: 0.975490 val_loss: 0.0539001\n",
      "1_275 train_acc: 0.9176 train_loss: 0.180999\tval_acc: 0.985294 val_loss: 0.0536568\n",
      "1_276 train_acc: 0.9288 train_loss: 0.203408\tval_acc: 0.985294 val_loss: 0.0426281\n",
      "1_309 train_acc: 0.9625 train_loss: 0.140289\tval_acc: 0.985294 val_loss: 0.0409594\n",
      "1_314 train_acc: 0.9401 train_loss: 0.158600\tval_acc: 0.990196 val_loss: 0.0364141\n",
      "1_322 train_acc: 0.9401 train_loss: 0.190334\tval_acc: 0.980392 val_loss: 0.0323052\n",
      "1_338 train_acc: 0.9288 train_loss: 0.190947\tval_acc: 0.990196 val_loss: 0.0270527\n",
      "1_346 train_acc: 0.9476 train_loss: 0.141334\tval_acc: 0.995098 val_loss: 0.0221837\n",
      "1_349 train_acc: 0.9213 train_loss: 0.165847\tval_acc: 0.995098 val_loss: 0.0206174\n",
      "1_363 train_acc: 0.9663 train_loss: 0.110485\tval_acc: 0.995098 val_loss: 0.0172774\n",
      "1_396 train_acc: 0.9401 train_loss: 0.112968\tval_acc: 1.000000 val_loss: 0.0129716\n",
      "1_421 train_acc: 0.9363 train_loss: 0.193328\tval_acc: 1.000000 val_loss: 0.0088357\n",
      "1_498 train_acc: 0.9625 train_loss: 0.121648\tval_acc: 1.000000 val_loss: 0.0077294\n",
      "1_506 train_acc: 0.9588 train_loss: 0.095751\tval_acc: 1.000000 val_loss: 0.0060158\n",
      "1_513 train_acc: 0.9551 train_loss: 0.153116\tval_acc: 1.000000 val_loss: 0.0050519\n",
      "1_572 train_acc: 0.9700 train_loss: 0.086144\tval_acc: 1.000000 val_loss: 0.0035675\n",
      "1_623 train_acc: 0.9625 train_loss: 0.137139\tval_acc: 1.000000 val_loss: 0.0031795\n",
      "1_625 train_acc: 0.9813 train_loss: 0.072881\tval_acc: 1.000000 val_loss: 0.0024484\n",
      "1_674 train_acc: 0.9625 train_loss: 0.104851\tval_acc: 1.000000 val_loss: 0.0015544\n",
      "1_776 train_acc: 0.9775 train_loss: 0.083993\tval_acc: 1.000000 val_loss: 0.0013855\n",
      "1_824 train_acc: 0.9625 train_loss: 0.097666\tval_acc: 1.000000 val_loss: 0.0009578\n",
      "1_836 train_acc: 0.9700 train_loss: 0.101439\tval_acc: 1.000000 val_loss: 0.0007423\n",
      "1_880 train_acc: 0.9625 train_loss: 0.112828\tval_acc: 1.000000 val_loss: 0.0006200\n",
      "1_925 train_acc: 0.9700 train_loss: 0.082473\tval_acc: 1.000000 val_loss: 0.0002790\n",
      "epoch:  925 \tThe test accuracy is: 0.8923611111111112\n",
      " THE BEST ACCURACY IS 0.8923611111111112\tkappa is 0.8564814814814815\n",
      "subject 1 duration: 0:08:24.264978\n",
      "seed is 1010\n",
      "Subject 2\n",
      "-------------------- train size： (288, 1, 22, 1000) test size： (288, 22, 1000)\n",
      "2_0 train_acc: 0.2172 train_loss: 2.492567\tval_acc: 0.299020 val_loss: 1.3631047\n",
      "2_1 train_acc: 0.3071 train_loss: 1.892682\tval_acc: 0.279412 val_loss: 1.3484980\n",
      "2_2 train_acc: 0.2772 train_loss: 2.026354\tval_acc: 0.367647 val_loss: 1.3144015\n",
      "2_3 train_acc: 0.2846 train_loss: 2.025913\tval_acc: 0.416667 val_loss: 1.2924407\n",
      "2_4 train_acc: 0.3296 train_loss: 1.750944\tval_acc: 0.431373 val_loss: 1.2844014\n",
      "2_5 train_acc: 0.3333 train_loss: 1.665062\tval_acc: 0.500000 val_loss: 1.2501115\n",
      "2_6 train_acc: 0.3184 train_loss: 1.619124\tval_acc: 0.539216 val_loss: 1.2136971\n",
      "2_7 train_acc: 0.3483 train_loss: 1.531192\tval_acc: 0.553922 val_loss: 1.2033825\n",
      "2_8 train_acc: 0.3071 train_loss: 1.599173\tval_acc: 0.563725 val_loss: 1.1877376\n",
      "2_9 train_acc: 0.3371 train_loss: 1.447965\tval_acc: 0.568627 val_loss: 1.1579685\n",
      "2_10 train_acc: 0.3221 train_loss: 1.467765\tval_acc: 0.622549 val_loss: 1.1530424\n",
      "2_11 train_acc: 0.3596 train_loss: 1.442777\tval_acc: 0.602941 val_loss: 1.1288581\n",
      "2_12 train_acc: 0.3371 train_loss: 1.495979\tval_acc: 0.617647 val_loss: 1.1171849\n",
      "2_13 train_acc: 0.3221 train_loss: 1.386588\tval_acc: 0.661765 val_loss: 1.0877490\n",
      "2_14 train_acc: 0.4419 train_loss: 1.233176\tval_acc: 0.607843 val_loss: 1.0799841\n",
      "2_15 train_acc: 0.3820 train_loss: 1.293562\tval_acc: 0.602941 val_loss: 1.0710753\n",
      "2_16 train_acc: 0.4307 train_loss: 1.242357\tval_acc: 0.681373 val_loss: 1.0298648\n",
      "2_17 train_acc: 0.4644 train_loss: 1.203847\tval_acc: 0.666667 val_loss: 0.9955173\n",
      "2_18 train_acc: 0.4719 train_loss: 1.161099\tval_acc: 0.710784 val_loss: 0.9777495\n",
      "2_19 train_acc: 0.4906 train_loss: 1.217647\tval_acc: 0.710784 val_loss: 0.9376846\n",
      "2_21 train_acc: 0.4906 train_loss: 1.144656\tval_acc: 0.671569 val_loss: 0.9059961\n",
      "2_22 train_acc: 0.5056 train_loss: 1.142836\tval_acc: 0.686275 val_loss: 0.8789975\n",
      "2_23 train_acc: 0.5993 train_loss: 1.035290\tval_acc: 0.725490 val_loss: 0.8567110\n",
      "2_25 train_acc: 0.5730 train_loss: 1.038060\tval_acc: 0.700980 val_loss: 0.8517848\n",
      "2_26 train_acc: 0.5843 train_loss: 1.048373\tval_acc: 0.735294 val_loss: 0.8044107\n",
      "2_27 train_acc: 0.5206 train_loss: 1.080897\tval_acc: 0.764706 val_loss: 0.7749417\n",
      "2_28 train_acc: 0.5393 train_loss: 1.073124\tval_acc: 0.750000 val_loss: 0.7673481\n",
      "2_32 train_acc: 0.6142 train_loss: 0.946453\tval_acc: 0.754902 val_loss: 0.7556343\n",
      "2_33 train_acc: 0.5431 train_loss: 1.075159\tval_acc: 0.750000 val_loss: 0.7525957\n",
      "2_34 train_acc: 0.5955 train_loss: 0.945522\tval_acc: 0.754902 val_loss: 0.7409663\n",
      "2_35 train_acc: 0.6217 train_loss: 0.943010\tval_acc: 0.750000 val_loss: 0.7274235\n",
      "2_37 train_acc: 0.5393 train_loss: 1.038142\tval_acc: 0.750000 val_loss: 0.7185722\n",
      "2_38 train_acc: 0.5880 train_loss: 0.999357\tval_acc: 0.735294 val_loss: 0.7181233\n",
      "2_39 train_acc: 0.6105 train_loss: 0.941425\tval_acc: 0.745098 val_loss: 0.6991298\n",
      "2_42 train_acc: 0.5880 train_loss: 0.961045\tval_acc: 0.769608 val_loss: 0.6947449\n",
      "2_44 train_acc: 0.6330 train_loss: 0.944303\tval_acc: 0.779412 val_loss: 0.6884006\n",
      "2_45 train_acc: 0.6142 train_loss: 0.901144\tval_acc: 0.769608 val_loss: 0.6820924\n",
      "2_48 train_acc: 0.6330 train_loss: 0.871749\tval_acc: 0.754902 val_loss: 0.6668877\n",
      "2_49 train_acc: 0.6592 train_loss: 0.807989\tval_acc: 0.813725 val_loss: 0.6143290\n",
      "2_54 train_acc: 0.6217 train_loss: 0.947972\tval_acc: 0.784314 val_loss: 0.6064215\n",
      "2_60 train_acc: 0.6966 train_loss: 0.761748\tval_acc: 0.818627 val_loss: 0.5832512\n",
      "2_61 train_acc: 0.6404 train_loss: 0.920834\tval_acc: 0.803922 val_loss: 0.5773780\n",
      "2_62 train_acc: 0.6554 train_loss: 0.850093\tval_acc: 0.799020 val_loss: 0.5679580\n",
      "2_63 train_acc: 0.6442 train_loss: 0.847385\tval_acc: 0.823529 val_loss: 0.5435306\n",
      "2_67 train_acc: 0.6854 train_loss: 0.772384\tval_acc: 0.813725 val_loss: 0.5430777\n",
      "2_75 train_acc: 0.6592 train_loss: 0.858768\tval_acc: 0.818627 val_loss: 0.5271749\n",
      "2_77 train_acc: 0.6667 train_loss: 0.775527\tval_acc: 0.848039 val_loss: 0.4720089\n",
      "2_80 train_acc: 0.6255 train_loss: 0.821883\tval_acc: 0.857843 val_loss: 0.4691418\n",
      "2_93 train_acc: 0.7154 train_loss: 0.769685\tval_acc: 0.843137 val_loss: 0.4478309\n",
      "2_94 train_acc: 0.7041 train_loss: 0.686333\tval_acc: 0.862745 val_loss: 0.4464430\n",
      "2_96 train_acc: 0.6816 train_loss: 0.768648\tval_acc: 0.872549 val_loss: 0.4098097\n",
      "2_100 train_acc: 0.7079 train_loss: 0.725480\tval_acc: 0.892157 val_loss: 0.4081300\n",
      "2_107 train_acc: 0.7940 train_loss: 0.574064\tval_acc: 0.852941 val_loss: 0.4014609\n",
      "2_109 train_acc: 0.6217 train_loss: 0.806202\tval_acc: 0.882353 val_loss: 0.3850485\n",
      "2_113 train_acc: 0.7079 train_loss: 0.705696\tval_acc: 0.892157 val_loss: 0.3660229\n",
      "2_116 train_acc: 0.6854 train_loss: 0.721931\tval_acc: 0.897059 val_loss: 0.3406699\n",
      "2_121 train_acc: 0.7378 train_loss: 0.661184\tval_acc: 0.892157 val_loss: 0.3358152\n",
      "2_130 train_acc: 0.7603 train_loss: 0.592269\tval_acc: 0.897059 val_loss: 0.3161515\n",
      "2_132 train_acc: 0.7191 train_loss: 0.661541\tval_acc: 0.897059 val_loss: 0.3157084\n",
      "2_134 train_acc: 0.7528 train_loss: 0.650653\tval_acc: 0.936275 val_loss: 0.2980907\n",
      "2_138 train_acc: 0.7678 train_loss: 0.588089\tval_acc: 0.906863 val_loss: 0.2686798\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 685\u001b[0m\n\u001b[1;32m    682\u001b[0m summary(sModel, (\u001b[38;5;241m1\u001b[39m, number_channel, \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28mprint\u001b[39m(time\u001b[38;5;241m.\u001b[39masctime(time\u001b[38;5;241m.\u001b[39mlocaltime(time\u001b[38;5;241m.\u001b[39mtime())))\n\u001b[0;32m--> 685\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRESULT_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m              \u001b[49m\u001b[43mevaluate_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEVALUATE_MODE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m              \u001b[49m\u001b[43mheads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHEADS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m              \u001b[49m\u001b[43memb_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEMB_DIM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m              \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEPTH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m              \u001b[49m\u001b[43mdataset_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m              \u001b[49m\u001b[43meeg1_f1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEEGNet1_F1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m              \u001b[49m\u001b[43meeg1_kernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEEGNet1_KERNEL_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m              \u001b[49m\u001b[43meeg1_D\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEEGNet1_D\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m              \u001b[49m\u001b[43meeg1_pooling_size1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEEGNet1_POOL_SIZE1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m              \u001b[49m\u001b[43meeg1_pooling_size2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEEGNet1_POOL_SIZE2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m              \u001b[49m\u001b[43meeg1_dropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEEGNet1_DROPOUT_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m              \u001b[49m\u001b[43mflatten_eeg1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLATTEN_EEGNet1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvalidate_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_ratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28mprint\u001b[39m(time\u001b[38;5;241m.\u001b[39masctime(time\u001b[38;5;241m.\u001b[39mlocaltime(time\u001b[38;5;241m.\u001b[39mtime())))\n",
      "Cell \u001b[0;32mIn[1], line 586\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(dirs, evaluate_mode, heads, emb_size, depth, dataset_type, eeg1_f1, eeg1_kernel_size, eeg1_D, eeg1_pooling_size1, eeg1_pooling_size2, eeg1_dropout_rate, flatten_eeg1, validate_ratio)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubject \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    571\u001b[0m exp \u001b[38;5;241m=\u001b[39m ExP(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, DATA_DIR, dirs, EPOCHS, N_AUG, N_SEG, gpus,\n\u001b[1;32m    572\u001b[0m           evaluate_mode \u001b[38;5;241m=\u001b[39m evaluate_mode,\n\u001b[1;32m    573\u001b[0m           heads\u001b[38;5;241m=\u001b[39mheads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    584\u001b[0m           validate_ratio\u001b[38;5;241m=\u001b[39mvalidate_ratio\n\u001b[1;32m    585\u001b[0m           )\n\u001b[0;32m--> 586\u001b[0m testAcc, Y_true, Y_pred, df_process, best_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m true_cpu \u001b[38;5;241m=\u001b[39m Y_true\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    588\u001b[0m pred_cpu \u001b[38;5;241m=\u001b[39m Y_pred\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 450\u001b[0m, in \u001b[0;36mExP.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    447\u001b[0m val_data_list\u001b[38;5;241m.\u001b[39mappend(img_batch[number_validate:])\n\u001b[1;32m    448\u001b[0m val_label_list\u001b[38;5;241m.\u001b[39mappend(label_batch[number_validate:])\n\u001b[0;32m--> 450\u001b[0m img_batch \u001b[38;5;241m=\u001b[39m Variable(\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    451\u001b[0m label_batch \u001b[38;5;241m=\u001b[39m Variable(train_label\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLongTensor))\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# data augmentation\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "CTNet with Mamba SSM instead of Transformer:\n",
    "Fully replicates your EEG-based motor imagery classification pipeline,\n",
    "but replaces the Transformer encoder blocks with Mamba SSM blocks.\n",
    "\n",
    "Author: zhaowei701@163.com\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "gpus = [0]\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from pandas import ExcelWriter\n",
    "from torchsummary import summary\n",
    "import torch\n",
    "from torch.backends import cudnn\n",
    "from utils import calMetrics\n",
    "from utils import calculatePerClass\n",
    "from utils import numberClassChannel\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from einops import rearrange, reduce, repeat\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import numberClassChannel\n",
    "from utils import load_data_evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "\n",
    "######################################\n",
    "# Try importing Mamba from mamba_ssm\n",
    "######################################\n",
    "try:\n",
    "    from mamba_ssm import Mamba\n",
    "except ImportError:\n",
    "    class Mamba(nn.Module):\n",
    "        \"\"\"\n",
    "        Dummy fallback if mamba_ssm is not installed.\n",
    "        This simply acts like a linear layer for demonstration.\n",
    "        \"\"\"\n",
    "        def __init__(self, d_model, d_state=16, d_conv=4, expand=2):\n",
    "            super().__init__()\n",
    "            self.linear = nn.Linear(d_model, d_model)\n",
    "        def forward(self, x):\n",
    "            # x shape: (batch, seq_len, d_model)\n",
    "            return self.linear(x)\n",
    "\n",
    "######################################\n",
    "# CNN Embedding (Patch Embedding)\n",
    "######################################\n",
    "class PatchEmbeddingCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Same as your original PatchEmbeddingCNN,\n",
    "    but no changes needed for Mamba-based architecture.\n",
    "    \"\"\"\n",
    "    def __init__(self, f1=16, kernel_size=64, D=2, pooling_size1=8, pooling_size2=8, \n",
    "                 dropout_rate=0.3, number_channel=22, emb_size=40):\n",
    "        super().__init__()\n",
    "        f2 = D * f1\n",
    "        self.cnn_module = nn.Sequential(\n",
    "            # Temporal conv\n",
    "            nn.Conv2d(1, f1, (1, kernel_size), (1, 1), padding='same', bias=False),\n",
    "            nn.BatchNorm2d(f1),\n",
    "            # Depthwise channel conv\n",
    "            nn.Conv2d(f1, f2, (number_channel, 1), (1, 1), groups=f1, padding='valid', bias=False),\n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "            # Average pooling 1\n",
    "            nn.AvgPool2d((1, pooling_size1)),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            # Additional spatial conv\n",
    "            nn.Conv2d(f2, f2, (1, 16), padding='same', bias=False),\n",
    "            nn.BatchNorm2d(f2),\n",
    "            nn.ELU(),\n",
    "            # Average pooling 2\n",
    "            nn.AvgPool2d((1, pooling_size2)),\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "\n",
    "        # Rearrange to (batch, seq_len, emb_size)\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b e h w -> b (h w) e'),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.cnn_module(x)           # -> (batch, f2, 1, new_time)\n",
    "        x = self.projection(x)           # -> (batch, seq_len, f2)\n",
    "        return x\n",
    "\n",
    "######################################\n",
    "# Simple Classification Head\n",
    "######################################\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, flatten_number, n_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(flatten_number, n_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "######################################\n",
    "# Positional Encoding (Optional)\n",
    "######################################\n",
    "class PositioinalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Same as your original positional encoding, if you want to\n",
    "    preserve the idea of 'time tokens' with added trainable positions.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding, length=100, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # trainable positional embeddings\n",
    "        self.encoding = nn.Parameter(torch.randn(1, length, embedding))\n",
    "    def forward(self, x):  # x -> [batch, seq_len, embedding]\n",
    "        # Add position enc up to x.shape[1]\n",
    "        x = x + self.encoding[:, : x.shape[1], :].cuda()\n",
    "        return self.dropout(x)\n",
    "\n",
    "######################################\n",
    "# Mamba Encoder Blocks\n",
    "######################################\n",
    "class MambaEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    One Mamba block that mirrors the Transformer sub-block structure:\n",
    "      1) LN -> Mamba -> Dropout -> residual\n",
    "      2) LN -> FeedForward -> Dropout -> residual\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_state=16, d_conv=4, expand=2, dropout=0.5, ff_expansion=4):\n",
    "        super().__init__()\n",
    "        # Sub-block 1: Mamba\n",
    "        self.mamba_norm = nn.LayerNorm(d_model)\n",
    "        self.mamba = Mamba(d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand)\n",
    "        self.mamba_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Sub-block 2: Feedforward\n",
    "        self.ffn_norm = nn.LayerNorm(d_model)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_expansion * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_expansion * d_model, d_model),\n",
    "        )\n",
    "        self.ffn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # (1) LN -> Mamba -> Dropout -> residual\n",
    "        y = self.mamba_norm(x)\n",
    "        y = self.mamba(y)\n",
    "        y = self.mamba_dropout(y)\n",
    "        x = x + y\n",
    "\n",
    "        # (2) LN -> Feedforward -> Dropout -> residual\n",
    "        z = self.ffn_norm(x)\n",
    "        z = self.feedforward(z)\n",
    "        z = self.ffn_dropout(z)\n",
    "        x = x + z\n",
    "        return x\n",
    "\n",
    "class MambaEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stacks multiple MambaEncoderBlocks.\n",
    "    \"\"\"\n",
    "    def __init__(self, depth=6, d_model=40, d_state=16, d_conv=4, expand=2, \n",
    "                 dropout=0.5, ff_expansion=4):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            MambaEncoderBlock(\n",
    "                d_model=d_model, d_state=d_state, d_conv=d_conv, expand=expand,\n",
    "                dropout=dropout, ff_expansion=ff_expansion\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # x shape: (batch, seq_len, d_model)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "######################################\n",
    "# \"EEGMamba\" Replacing the Transformer\n",
    "######################################\n",
    "class EEGMamba(nn.Module):\n",
    "    \"\"\"\n",
    "    This replicates your EEGTransformer, but uses Mamba blocks instead of\n",
    "    Transformer blocks (multi-head attention).\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 emb_size=40,\n",
    "                 depth=6,\n",
    "                 database_type='A',\n",
    "                 eeg1_f1=20,\n",
    "                 eeg1_kernel_size=64,\n",
    "                 eeg1_D=2,\n",
    "                 eeg1_pooling_size1=8,\n",
    "                 eeg1_pooling_size2=8,\n",
    "                 eeg1_dropout_rate=0.3,\n",
    "                 eeg1_number_channel=22,\n",
    "                 flatten_eeg1=600,\n",
    "                 d_state=16, \n",
    "                 d_conv=4,\n",
    "                 expand=2,\n",
    "                 ff_expansion=4,\n",
    "                 dropout_mamba=0.5,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        # number of classes\n",
    "        self.number_class, _ = numberClassChannel(database_type)\n",
    "        self.emb_size = emb_size\n",
    "        self.flatten_eeg1 = flatten_eeg1\n",
    "\n",
    "        # CNN embedding\n",
    "        self.cnn = PatchEmbeddingCNN(\n",
    "            f1=eeg1_f1,\n",
    "            kernel_size=eeg1_kernel_size,\n",
    "            D=eeg1_D,\n",
    "            pooling_size1=eeg1_pooling_size1,\n",
    "            pooling_size2=eeg1_pooling_size2,\n",
    "            dropout_rate=eeg1_dropout_rate,\n",
    "            number_channel=eeg1_number_channel,\n",
    "            emb_size=emb_size\n",
    "        )\n",
    "\n",
    "        # Positional encoding\n",
    "        self.position = PositioinalEncoding(emb_size, dropout=0.1)\n",
    "\n",
    "        # Mamba Encoder\n",
    "        self.mamba_encoder = MambaEncoder(\n",
    "            depth=depth,\n",
    "            d_model=emb_size,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand,\n",
    "            dropout=dropout_mamba,\n",
    "            ff_expansion=ff_expansion\n",
    "        )\n",
    "\n",
    "        # Flatten + Classification\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classification = ClassificationHead(self.flatten_eeg1, self.number_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN embedding -> (batch, seq_len, emb_size)\n",
    "        cnn = self.cnn(x)\n",
    "\n",
    "        # scale\n",
    "        cnn = cnn * math.sqrt(self.emb_size)\n",
    "\n",
    "        # add positional encoding\n",
    "        cnn = self.position(cnn)\n",
    "\n",
    "        # Mamba\n",
    "        features = self.mamba_encoder(cnn)\n",
    "\n",
    "        # final classification\n",
    "        out = self.classification(self.flatten(features))\n",
    "        return features, out\n",
    "\n",
    "######################################\n",
    "# The rest of your pipeline remains unchanged\n",
    "######################################\n",
    "\n",
    "class ExP():\n",
    "    def __init__(self, nsub, data_dir, result_name, \n",
    "                 epochs=2000, \n",
    "                 number_aug=2,\n",
    "                 number_seg=8, \n",
    "                 gpus=[0], \n",
    "                 evaluate_mode = 'subject-dependent',\n",
    "                 heads=4, \n",
    "                 emb_size=40,\n",
    "                 depth=6, \n",
    "                 dataset_type='A',\n",
    "                 eeg1_f1 = 20,\n",
    "                 eeg1_kernel_size = 64,\n",
    "                 eeg1_D = 2,\n",
    "                 eeg1_pooling_size1 = 8,\n",
    "                 eeg1_pooling_size2 = 8,\n",
    "                 eeg1_dropout_rate = 0.3,\n",
    "                 flatten_eeg1 = 600, \n",
    "                 validate_ratio = 0.2,\n",
    "                 learning_rate = 0.001,\n",
    "                 batch_size = 72,  \n",
    "                 ):\n",
    "        \n",
    "        super(ExP, self).__init__()\n",
    "        self.dataset_type = dataset_type\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = learning_rate\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.n_epochs = epochs\n",
    "        self.nSub = nsub\n",
    "        self.number_augmentation = number_aug\n",
    "        self.number_seg = number_seg\n",
    "        self.root = data_dir\n",
    "        self.heads = heads\n",
    "        self.emb_size = emb_size\n",
    "        self.depth = depth\n",
    "        self.result_name = result_name\n",
    "        self.evaluate_mode = evaluate_mode\n",
    "        self.validate_ratio = validate_ratio\n",
    "\n",
    "        self.Tensor = torch.cuda.FloatTensor\n",
    "        self.LongTensor = torch.cuda.LongTensor\n",
    "        self.criterion_cls = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "        self.number_class, self.number_channel = numberClassChannel(self.dataset_type)\n",
    "        \n",
    "        #  >>> REPLACE EEGTransformer with EEGMamba <<<\n",
    "        self.model = EEGMamba(\n",
    "            emb_size=self.emb_size,\n",
    "            depth=self.depth, \n",
    "            database_type=self.dataset_type,\n",
    "            eeg1_f1=eeg1_f1,\n",
    "            eeg1_kernel_size=eeg1_kernel_size,\n",
    "            eeg1_D=eeg1_D,\n",
    "            eeg1_pooling_size1=eeg1_pooling_size1,\n",
    "            eeg1_pooling_size2=eeg1_pooling_size2,\n",
    "            eeg1_dropout_rate=eeg1_dropout_rate,\n",
    "            eeg1_number_channel=self.number_channel,\n",
    "            flatten_eeg1=flatten_eeg1,\n",
    "        ).cuda()\n",
    "        self.model_filename = self.result_name + '/model_{}.pth'.format(self.nSub)\n",
    "\n",
    "\n",
    "    def interaug(self, timg, label):  \n",
    "        aug_data = []\n",
    "        aug_label = []\n",
    "        number_records_by_augmentation = self.number_augmentation * int(self.batch_size / self.number_class)\n",
    "        number_segmentation_points = 1000 // self.number_seg\n",
    "        for clsAug in range(self.number_class):\n",
    "            cls_idx = np.where(label == clsAug + 1)\n",
    "            tmp_data = timg[cls_idx]\n",
    "            tmp_label = label[cls_idx]\n",
    "            \n",
    "            tmp_aug_data = np.zeros((number_records_by_augmentation, 1, self.number_channel, 1000))\n",
    "            for ri in range(number_records_by_augmentation):\n",
    "                for rj in range(self.number_seg):\n",
    "                    rand_idx = np.random.randint(0, tmp_data.shape[0], self.number_seg)\n",
    "                    tmp_aug_data[ri, :, :, rj * number_segmentation_points:(rj + 1) * number_segmentation_points] = \\\n",
    "                        tmp_data[rand_idx[rj], :, :, rj * number_segmentation_points:(rj + 1) * number_segmentation_points]\n",
    "\n",
    "            aug_data.append(tmp_aug_data)\n",
    "            aug_label.append(tmp_label[:number_records_by_augmentation])\n",
    "        aug_data = np.concatenate(aug_data)\n",
    "        aug_label = np.concatenate(aug_label)\n",
    "        aug_shuffle = np.random.permutation(len(aug_data))\n",
    "        aug_data = aug_data[aug_shuffle, :, :]\n",
    "        aug_label = aug_label[aug_shuffle]\n",
    "\n",
    "        aug_data = torch.from_numpy(aug_data).cuda().float()\n",
    "        aug_label = torch.from_numpy(aug_label-1).cuda().long()\n",
    "        return aug_data, aug_label\n",
    "\n",
    "    def get_source_data(self):\n",
    "        (self.train_data,\n",
    "         self.train_label, \n",
    "         self.test_data, \n",
    "         self.test_label) = load_data_evaluate(self.root, self.dataset_type, self.nSub, mode_evaluate=self.evaluate_mode)\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data, axis=1)\n",
    "        self.train_label = np.transpose(self.train_label)\n",
    "        self.allData = self.train_data\n",
    "        self.allLabel = self.train_label[0]\n",
    "\n",
    "        shuffle_num = np.random.permutation(len(self.allData))\n",
    "        self.allData = self.allData[shuffle_num, :, :, :]\n",
    "        self.allLabel = self.allLabel[shuffle_num]\n",
    "\n",
    "        print('-'*20, \"train size：\", self.train_data.shape, \"test size：\", self.test_data.shape)\n",
    "        self.test_data = np.expand_dims(self.test_data, axis=1)\n",
    "        self.test_label = np.transpose(self.test_label)\n",
    "\n",
    "        self.testData = self.test_data\n",
    "        self.testLabel = self.test_label[0]\n",
    "\n",
    "        # standardize\n",
    "        target_mean = np.mean(self.allData)\n",
    "        target_std = np.std(self.allData)\n",
    "        self.allData = (self.allData - target_mean) / target_std\n",
    "        self.testData = (self.testData - target_mean) / target_std\n",
    "        \n",
    "        return self.allData, self.allLabel, self.testData, self.testLabel\n",
    "\n",
    "    def train(self):\n",
    "        img, label, test_data, test_label = self.get_source_data()\n",
    "        \n",
    "        img = torch.from_numpy(img)\n",
    "        label = torch.from_numpy(label - 1)\n",
    "        dataset = torch.utils.data.TensorDataset(img, label)\n",
    "        \n",
    "        test_data = torch.from_numpy(test_data)\n",
    "        test_label = torch.from_numpy(test_label - 1)\n",
    "        test_dataset = torch.utils.data.TensorDataset(test_data, test_label)\n",
    "        self.test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
    "\n",
    "        test_data = Variable(test_data.type(self.Tensor))\n",
    "        test_label = Variable(test_label.type(self.LongTensor))\n",
    "        best_epoch = 0\n",
    "        num = 0\n",
    "        min_loss = 100\n",
    "        result_process = []\n",
    "        for e in range(self.n_epochs):\n",
    "            self.dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=self.batch_size, shuffle=True)\n",
    "            epoch_process = {}\n",
    "            epoch_process['epoch'] = e\n",
    "            self.model.train()\n",
    "            outputs_list = []\n",
    "            label_list = []\n",
    "\n",
    "            val_data_list = []\n",
    "            val_label_list = []\n",
    "\n",
    "            for i, (img_batch, label_batch) in enumerate(self.dataloader):\n",
    "                number_sample = img_batch.shape[0]\n",
    "                number_validate = int(self.validate_ratio * number_sample)\n",
    "\n",
    "                # split real train / validate\n",
    "                train_data = img_batch[:-number_validate]\n",
    "                train_label = label_batch[:-number_validate]\n",
    "\n",
    "                val_data_list.append(img_batch[number_validate:])\n",
    "                val_label_list.append(label_batch[number_validate:])\n",
    "\n",
    "                img_batch = Variable(train_data.type(self.Tensor))\n",
    "                label_batch = Variable(train_label.type(self.LongTensor))\n",
    "\n",
    "                # data augmentation\n",
    "                aug_data, aug_label = self.interaug(self.allData, self.allLabel)\n",
    "                # concat\n",
    "                img_batch = torch.cat((img_batch, aug_data))\n",
    "                label_batch = torch.cat((label_batch, aug_label))\n",
    "\n",
    "                features, outputs = self.model(img_batch)\n",
    "                outputs_list.append(outputs)\n",
    "                label_list.append(label_batch)\n",
    "                loss = self.criterion_cls(outputs, label_batch)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # validate\n",
    "            self.model.eval()\n",
    "            val_data = torch.cat(val_data_list).cuda()\n",
    "            val_label = torch.cat(val_label_list).cuda()\n",
    "            val_data = val_data.type(self.Tensor)\n",
    "            val_label = val_label.type(self.LongTensor)\n",
    "\n",
    "            val_dataset = torch.utils.data.TensorDataset(val_data, val_label)\n",
    "            val_dataloader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "            outputs_list = []\n",
    "            with torch.no_grad():\n",
    "                for i, (img_v, _) in enumerate(val_dataloader):\n",
    "                    img_v = img_v.type(self.Tensor).cuda()\n",
    "                    _, Cls = self.model(img_v)\n",
    "                    outputs_list.append(Cls)\n",
    "            Cls = torch.cat(outputs_list)\n",
    "\n",
    "            val_loss = self.criterion_cls(Cls, val_label)\n",
    "            val_pred = torch.max(Cls, 1)[1]\n",
    "            val_acc = float((val_pred == val_label).cpu().numpy().sum()) / float(val_label.size(0))\n",
    "\n",
    "            epoch_process['val_acc'] = val_acc\n",
    "            epoch_process['val_loss'] = val_loss.detach().cpu().numpy()\n",
    "\n",
    "            # compute training acc from last batch\n",
    "            train_pred = torch.max(outputs, 1)[1]\n",
    "            train_acc = float((train_pred == label_batch).cpu().numpy().sum()) / float(label_batch.size(0))\n",
    "            epoch_process['train_acc'] = train_acc\n",
    "            epoch_process['train_loss'] = loss.detach().cpu().numpy()\n",
    "\n",
    "            if val_loss < min_loss:\n",
    "                min_loss = val_loss\n",
    "                best_epoch = e\n",
    "                torch.save(self.model, self.model_filename)\n",
    "                print(\"{}_{} train_acc: {:.4f} train_loss: {:.6f}\\tval_acc: {:.6f} val_loss: {:.7f}\".format(\n",
    "                    self.nSub, epoch_process['epoch'], epoch_process['train_acc'],\n",
    "                    epoch_process['train_loss'], epoch_process['val_acc'], epoch_process['val_loss']\n",
    "                ))\n",
    "\n",
    "            result_process.append(epoch_process)\n",
    "\n",
    "        # load best model for final test\n",
    "        self.model.eval()\n",
    "        from torch.serialization import safe_globals\n",
    "        with safe_globals([EEGMamba, PatchEmbeddingCNN, PositioinalEncoding, ClassificationHead, \n",
    "                            MambaEncoder, MambaEncoderBlock, nn.Sequential]):\n",
    "            self.model = torch.load(self.model_filename, weights_only=False)\n",
    "        self.model = self.model.cuda()\n",
    "\n",
    "        outputs_list = []\n",
    "        with torch.no_grad():\n",
    "            for i, (img_batch, label_batch) in enumerate(self.test_dataloader):\n",
    "                img_test = Variable(img_batch.type(self.Tensor)).cuda()\n",
    "                features, outputs = self.model(img_test)\n",
    "                outputs_list.append(outputs)\n",
    "        outputs = torch.cat(outputs_list)\n",
    "        y_pred = torch.max(outputs, 1)[1]\n",
    "\n",
    "        test_acc = float((y_pred == test_label).cpu().numpy().sum()) / float(test_label.size(0))\n",
    "        print(\"epoch: \", best_epoch, '\\tThe test accuracy is:', test_acc)\n",
    "\n",
    "        df_process = pd.DataFrame(result_process)\n",
    "        return test_acc, test_label, y_pred, df_process, best_epoch\n",
    "\n",
    "\n",
    "def main(dirs,                \n",
    "         evaluate_mode = 'subject-dependent',\n",
    "         heads=8,           \n",
    "         emb_size=48,       \n",
    "         depth=3,           \n",
    "         dataset_type='A',  \n",
    "         eeg1_f1=20,\n",
    "         eeg1_kernel_size=64,\n",
    "         eeg1_D=2,\n",
    "         eeg1_pooling_size1=8,\n",
    "         eeg1_pooling_size2=8,\n",
    "         eeg1_dropout_rate=0.3,\n",
    "         flatten_eeg1=600,   \n",
    "         validate_ratio=0.2\n",
    "         ):\n",
    "\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "\n",
    "    result_write_metric = ExcelWriter(dirs+\"/result_metric.xlsx\")\n",
    "    process_write = ExcelWriter(dirs+\"/process_train.xlsx\")\n",
    "    pred_true_write = ExcelWriter(dirs+\"/pred_true.xlsx\")\n",
    "\n",
    "    result_metric_dict = {}\n",
    "    y_true_pred_dict = {}\n",
    "    subjects_result = []\n",
    "    best_epochs = []\n",
    "\n",
    "    for i in range(N_SUBJECT):\n",
    "        starttime = datetime.datetime.now()\n",
    "        seed_n = np.random.randint(2024)\n",
    "        print('seed is ' + str(seed_n))\n",
    "        random.seed(seed_n)\n",
    "        np.random.seed(seed_n)\n",
    "        torch.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed(seed_n)\n",
    "        torch.cuda.manual_seed_all(seed_n)\n",
    "\n",
    "        print('Subject %d' % (i+1))\n",
    "        exp = ExP(i + 1, DATA_DIR, dirs, EPOCHS, N_AUG, N_SEG, gpus,\n",
    "                  evaluate_mode = evaluate_mode,\n",
    "                  heads=heads,\n",
    "                  emb_size=emb_size,\n",
    "                  depth=depth,\n",
    "                  dataset_type=dataset_type,\n",
    "                  eeg1_f1=eeg1_f1,\n",
    "                  eeg1_kernel_size=eeg1_kernel_size,\n",
    "                  eeg1_D=eeg1_D,\n",
    "                  eeg1_pooling_size1=eeg1_pooling_size1,\n",
    "                  eeg1_pooling_size2=eeg1_pooling_size2,\n",
    "                  eeg1_dropout_rate=eeg1_dropout_rate,\n",
    "                  flatten_eeg1=flatten_eeg1,\n",
    "                  validate_ratio=validate_ratio\n",
    "                  )\n",
    "        testAcc, Y_true, Y_pred, df_process, best_epoch = exp.train()\n",
    "        true_cpu = Y_true.cpu().numpy().astype(int)\n",
    "        pred_cpu = Y_pred.cpu().numpy().astype(int)\n",
    "        df_pred_true = pd.DataFrame({'pred': pred_cpu, 'true': true_cpu})\n",
    "        df_pred_true.to_excel(pred_true_write, sheet_name=str(i+1))\n",
    "        y_true_pred_dict[i] = df_pred_true\n",
    "\n",
    "        accuracy, precison, recall, f1, kappa = calMetrics(true_cpu, pred_cpu)\n",
    "        subject_result = {\n",
    "            'accuray': accuracy*100,\n",
    "            'precision': precison*100,\n",
    "            'recall': recall*100,\n",
    "            'f1': f1*100,\n",
    "            'kappa': kappa*100\n",
    "        }\n",
    "        subjects_result.append(subject_result)\n",
    "        df_process.to_excel(process_write, sheet_name=str(i+1))\n",
    "        best_epochs.append(best_epoch)\n",
    "    \n",
    "        print(' THE BEST ACCURACY IS ' + str(testAcc) + \"\\tkappa is \" + str(kappa) )\n",
    "        endtime = datetime.datetime.now()\n",
    "        print('subject %d duration: '%(i+1) + str(endtime - starttime))\n",
    "\n",
    "        if i == 0:\n",
    "            yt = Y_true\n",
    "            yp = Y_pred\n",
    "        else:\n",
    "            yt = torch.cat((yt, Y_true))\n",
    "            yp = torch.cat((yp, Y_pred))\n",
    "\n",
    "    df_result = pd.DataFrame(subjects_result)\n",
    "    process_write.close()\n",
    "    pred_true_write.close()\n",
    "\n",
    "    print('**The average Best accuracy is: ' + str(df_result['accuray'].mean()) + \n",
    "          \"kappa is: \" + str(df_result['kappa'].mean()) + \"\\n\" )\n",
    "    print(\"best epochs: \", best_epochs)\n",
    "\n",
    "    mean = df_result.mean(axis=0)\n",
    "    mean.name = 'mean'\n",
    "    std = df_result.std(axis=0)\n",
    "    std.name = 'std'\n",
    "    df_result = pd.concat([df_result, pd.DataFrame(mean).T, pd.DataFrame(std).T])\n",
    "    df_result.to_excel(result_write_metric, index=False)\n",
    "\n",
    "    print('-'*9, ' all result ', '-'*9)\n",
    "    print(df_result)\n",
    "    print(\"*\"*40)\n",
    "    result_write_metric.close()\n",
    "\n",
    "    return df_result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    DATA_DIR = r'bci2a/'\n",
    "    EVALUATE_MODE = 'LOSO-No'\n",
    "    N_SUBJECT = 9\n",
    "    N_AUG = 3\n",
    "    N_SEG = 8\n",
    "    EPOCHS = 1000\n",
    "    EMB_DIM = 16\n",
    "    HEADS = 2\n",
    "    DEPTH = 6\n",
    "    TYPE = 'B'\n",
    "    validate_ratio = 0.3\n",
    "\n",
    "    EEGNet1_F1 = 8\n",
    "    EEGNet1_KERNEL_SIZE=64\n",
    "    EEGNet1_D=2\n",
    "    EEGNet1_POOL_SIZE1 = 8\n",
    "    EEGNet1_POOL_SIZE2 = 8\n",
    "    FLATTEN_EEGNet1 = 240\n",
    "    if EVALUATE_MODE!='LOSO':\n",
    "        EEGNet1_DROPOUT_RATE = 0.5\n",
    "    else:\n",
    "        EEGNet1_DROPOUT_RATE = 0.25    \n",
    "\n",
    "    parameters_list = ['A']\n",
    "    for TYPE in parameters_list:\n",
    "        number_class, number_channel = numberClassChannel(TYPE)\n",
    "        RESULT_NAME = \"CTNetMamba_{}_heads_{}_depth_{}_{}\".format(TYPE, HEADS, DEPTH, int(time.time()))\n",
    "\n",
    "        sModel = EEGMamba(\n",
    "            emb_size=EMB_DIM,\n",
    "            depth=DEPTH,\n",
    "            database_type=TYPE,\n",
    "            eeg1_f1=EEGNet1_F1,\n",
    "            eeg1_D=EEGNet1_D,\n",
    "            eeg1_kernel_size=EEGNet1_KERNEL_SIZE,\n",
    "            eeg1_pooling_size1=EEGNet1_POOL_SIZE1,\n",
    "            eeg1_pooling_size2=EEGNet1_POOL_SIZE2,\n",
    "            eeg1_dropout_rate=EEGNet1_DROPOUT_RATE,\n",
    "            eeg1_number_channel=number_channel,\n",
    "            flatten_eeg1=FLATTEN_EEGNet1,\n",
    "        ).cuda()\n",
    "        summary(sModel, (1, number_channel, 1000))\n",
    "\n",
    "        print(time.asctime(time.localtime(time.time())))\n",
    "        result = main(RESULT_NAME,\n",
    "                      evaluate_mode=EVALUATE_MODE,\n",
    "                      heads=HEADS,\n",
    "                      emb_size=EMB_DIM,\n",
    "                      depth=DEPTH,\n",
    "                      dataset_type=TYPE,\n",
    "                      eeg1_f1=EEGNet1_F1,\n",
    "                      eeg1_kernel_size=EEGNet1_KERNEL_SIZE,\n",
    "                      eeg1_D=EEGNet1_D,\n",
    "                      eeg1_pooling_size1=EEGNet1_POOL_SIZE1,\n",
    "                      eeg1_pooling_size2=EEGNet1_POOL_SIZE2,\n",
    "                      eeg1_dropout_rate=EEGNet1_DROPOUT_RATE,\n",
    "                      flatten_eeg1=FLATTEN_EEGNet1,\n",
    "                      validate_ratio=validate_ratio)\n",
    "        print(time.asctime(time.localtime(time.time())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed08c147-3121-47d3-b9f3-e09310aa1f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mamba_working)",
   "language": "python",
   "name": "mamba_working"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
